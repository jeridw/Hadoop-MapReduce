{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5a187c",
   "metadata": {},
   "source": [
    "## 1. Mapper Script for Wind Speed Difference, Minimum Relative Humidity, and Dew Point Temp Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea170a",
   "metadata": {},
   "source": [
    "##Â 1.1 Uplading the dataset in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a88b8c",
   "metadata": {},
   "source": [
    "First I will create a new directory called weather on HDFS using mkdir and then navigate to it using the cd command. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "[xxx@dsm1 ~]$ mkdir weather\n",
    "[xxx@dsm1 ~]$ cd weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next I will copy the data from local disk to the cluster\n",
    "scp \"/Users/jerid/Desktop/200707hourly.txt\" xxx@dsm1.doc.gold.ac.uk:~/weather/\n",
    "\n",
    "#To verify it is on the cluster I will use the ls command\n",
    "\n",
    "[xxx@dsm1 weather]$ ls\n",
    "\n",
    "200707hourly.txt \n",
    "    \n",
    "# I can see it exists and now I will copy the file onto HDFS\n",
    "hadoop fs -copyFromLocal weather/200707hourly.txt\n",
    "\n",
    "#Verifying that its on HDFS\n",
    "\n",
    "[xxx@dsm1 ~]$ hadoop fs -ls /user/xxx/\n",
    "Found 20 items\n",
    "-rw-r--r--   3 xxx hadoop  101449102 2024-03-07 10:32 /user/xxx/200707hourl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648be316",
   "metadata": {},
   "source": [
    "## 1.2 Testing the mapper on a smaller file in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aeaa91",
   "metadata": {},
   "source": [
    "Firstly, I will create smaller file of 100 lines. I have commented it out since I have already ran it. I created it for testing purposes as it will make the testing process quicker and help with debugging issues. Once I am confident with the scripts perfomance in python, I will then run this on the full dataset in hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a046287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_lines = 100\n",
    "\n",
    "#with open('200707hourly.txt', 'r') as infile, open('200707hourly_tiny.txt', 'w') as outfile:\n",
    "    #for i, line in enumerate(tpinfile):\n",
    "        #if i >= num_lines:\n",
    "            #break\n",
    "        #outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963af4a7",
   "metadata": {},
   "source": [
    "First, I will create the mapper on smaller file in python and test the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557ab861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file for reading and the mapper_output file for writing\n",
    "\n",
    "with open(\"200707hourly_tiny.txt\", \"r\") as file, open(\"mapper_output.txt\", \"w\") as outfile:\n",
    "    for line in file:\n",
    "        # I will split the line into columns\n",
    "        \n",
    "        columns = line.strip().split(',')\n",
    "        \n",
    "        try:\n",
    "            # Next I will extract the necessary fields\n",
    "            \n",
    "            date = columns[1]\n",
    "            wind_speed = float(columns[12])  # python reads first column as 0 so wind speed is at index 12\n",
    "            relative_humidity = float(columns[11].strip('%'))  # I need to remove the '%' in relative humidity \n",
    "            dew_point_temp = float(columns[9])  \n",
    "            \n",
    "            # I will need to emit data for wind speed analysis.\n",
    "            outfile.write(f\"{date}_WS\\t{wind_speed}\\n\") \n",
    "            \n",
    "            # I will also emit data for relative humidity analysis \n",
    "            outfile.write(f\"{date}_RH\\t{relative_humidity}\\n\")\n",
    "            \n",
    "            # Finally, I will emit data for dew point temperature analysis \n",
    "            outfile.write(f\"{date}_DPT\\t{dew_point_temp}\\n\")\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40034c9",
   "metadata": {},
   "source": [
    "I will now open and read the first few lines of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31eba956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701_WS\t3.0\n",
      "20070701_RH\t22.0\n",
      "20070701_DPT\t18.0\n",
      "20070701_WS\t3.0\n",
      "20070701_RH\t23.0\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "\n",
    "with open(\"mapper_output.txt\", \"r\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        print(line.strip())\n",
    "        if i >= 4:  \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f2874",
   "metadata": {},
   "source": [
    "Now that it works smoothly, I will create the mapper.py file within the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the mapper file using touch command\n",
    "touch mapper.py\n",
    "\n",
    "# Opening the mapper file with nano and will then paste the code below and save\n",
    "nano mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19960fca",
   "metadata": {},
   "source": [
    "Within my nano mapper file created I will copy the code below as Hadoop, mappers and reducers communicate with stdin and stdout and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e55bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    columns = line.strip().split(',')\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        date = columns[1]\n",
    "        wind_speed = float(columns[12]) \n",
    "        relative_humidity = float(columns[11].strip('%')) \n",
    "        dew_point_temp = float(columns[9])  \n",
    "        \n",
    "       \n",
    "        print(f\"{date}_WS\\t{wind_speed}\")\n",
    "        \n",
    "        \n",
    "        print(f\"{date}_RH\\t{relative_humidity}\")\n",
    "        \n",
    "        print(f\"{date}_DPT\\t{dew_point_temp}\")\n",
    "        \n",
    "    except ValueError:\n",
    "        \n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21750d",
   "metadata": {},
   "source": [
    "## 2. Reducer Script for Maximum and Minimum Wind Speed Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ae316",
   "metadata": {},
   "source": [
    "Now the mapper is created, I will derive the min, max etc in the reducers below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af474a",
   "metadata": {},
   "source": [
    "## 2.1 Calculating Wind Speed Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122aef49",
   "metadata": {},
   "source": [
    "Testing on small file in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8089e2df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t9.0\n",
      "20070702\t7.0\n",
      "20070703\t7.0\n",
      "20070704\t11.0\n",
      "20070705\t0.0\n"
     ]
    }
   ],
   "source": [
    "file = open(\"mapper_output.txt\", \"r\")\n",
    "\n",
    "current_date = None\n",
    "max_wind_speed = float('-inf')\n",
    "min_wind_speed = float('inf')\n",
    "\n",
    "for line in file.readlines():\n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    value = float(value)\n",
    "\n",
    "    if date_key.endswith(\"_WS\"):  # I will only process wind speed data\n",
    "        date = date_key.split('_')[0]\n",
    "\n",
    "        if date == current_date:\n",
    "            max_wind_speed = max(max_wind_speed, value)\n",
    "            min_wind_speed = min(min_wind_speed, value)\n",
    "        else:\n",
    "            if current_date:\n",
    "                # I will output the difference for the previous date\n",
    "                print(f\"{current_date}\\t{max_wind_speed - min_wind_speed}\")\n",
    "            current_date = date\n",
    "            max_wind_speed = min_wind_speed = value\n",
    "\n",
    "# I will output the last date after the loop\n",
    "if current_date:\n",
    "    print(f\"{current_date}\\t{max_wind_speed - min_wind_speed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba21433",
   "metadata": {},
   "source": [
    "Since it is successful I will create the py file within the cluster and copy in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf286209",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch reducer_max_min.py \n",
    "nano reducer_max_min.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e07ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_date = None\n",
    "max_wind_speed = float('-inf')\n",
    "min_wind_speed = float('inf')\n",
    "\n",
    "for line in sys.stdin:\n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    value = float(value)\n",
    "    \n",
    "    if date_key.endswith(\"_WS\"):  \n",
    "        date = date_key.split('_')[0]\n",
    "        \n",
    "        if date == current_date:\n",
    "            max_wind_speed = max(max_wind_speed, value)\n",
    "            min_wind_speed = min(min_wind_speed, value)\n",
    "        else:\n",
    "            if current_date:\n",
    "                \n",
    "                print(f\"{current_date}\\t{max_wind_speed - min_wind_speed}\")\n",
    "            \n",
    "            current_date = date\n",
    "            max_wind_speed = min_wind_speed = value\n",
    "\n",
    "\n",
    "if current_date:\n",
    "    print(f\"{current_date}\\t{max_wind_speed - min_wind_speed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the mapper and reducer on the hadoop cluster\n",
    "\n",
    "[jdawi001@dsm1 weather]$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "    -files mapper.py,reducer_max_min.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer_max_min.py\" \\\n",
    "    -input /user/jdawi001/200707hourly.txt -output /user/jdawi001/200707_windspeed_difference\n",
    "\n",
    "\n",
    "# Copy the output from HDFS onto the headnode\n",
    "\n",
    "hadoop fs -copyToLocal /user/jdawi001/200707_windspeed_difference\n",
    "\n",
    "# Download the output from the headnode onto local machine\n",
    "\n",
    "scp -r jdawi001@dsm1.doc.gold.ac.uk:~/weather/200707_windspeed_difference /Users/jerid/Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ddbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results of Wind Speed Difference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32aeeff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t31.0\n",
      "20070702\t80.0\n",
      "20070703\t37.0\n",
      "20070704\t29.0\n",
      "20070705\t37.0\n",
      "20070706\t64.0\n",
      "20070707\t51.0\n",
      "20070708\t37.0\n",
      "20070709\t36.0\n",
      "20070710\t82.0\n",
      "20070711\t37.0\n",
      "20070712\t32.0\n",
      "20070713\t31.0\n",
      "20070714\t30.0\n",
      "20070715\t36.0\n",
      "20070716\t35.0\n",
      "20070717\t35.0\n",
      "20070718\t39.0\n",
      "20070719\t12.0\n"
     ]
    }
   ],
   "source": [
    "file = open('weather_final_results/200707_windspeed_difference/part-00000', 'r', encoding='UTF-8')\n",
    "\n",
    "for line in file.readlines():\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfff17",
   "metadata": {},
   "source": [
    "## 2.2 Calculating Minimum relative humidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b95928",
   "metadata": {},
   "source": [
    "Testing reducer on a smaller file within Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b45611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t3.0\n",
      "20070702\t3.0\n",
      "20070703\t3.0\n",
      "20070704\t5.0\n",
      "20070705\t36.0\n"
     ]
    }
   ],
   "source": [
    "file = open(\"mapper_output.txt\", \"r\")\n",
    "\n",
    "current_date = None\n",
    "min_relative_humidity = float('inf')\n",
    "\n",
    "for line in file.readlines():\n",
    "    # I will split the line by tab to get the date key and the relative humidity value\n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    value = float(value)  # I also need to convert the string to float to compare\n",
    "    \n",
    "    if date_key.endswith(\"_RH\"):  # I will use RH as I am processing only relative humidity data\n",
    "        date = date_key.split('_')[0]\n",
    "        \n",
    "        if date == current_date:\n",
    "            # I will update min if the current value is less than the current minimum\n",
    "            min_relative_humidity = min(min_relative_humidity, value)\n",
    "        else:\n",
    "            # If it moves to a new date, I will output the minimum for the previous date, if not the first line.\n",
    "            if current_date is not None:\n",
    "                print(f\"{current_date}\\t{min_relative_humidity}\")\n",
    "            \n",
    "            # I will now reset for the new date\n",
    "            current_date = date\n",
    "            min_relative_humidity = value\n",
    "\n",
    "# Now I must output the last date after finishing all the lines\n",
    "if current_date:\n",
    "    print(f\"{current_date}\\t{min_relative_humidity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd647608",
   "metadata": {},
   "source": [
    "I will now create the py file within the cluster and copy in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92135c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch reducer_min.py\n",
    "nano reducer_min.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_date = None\n",
    "min_relative_humidity = float('inf')\n",
    "\n",
    "for line in sys.stdin:\n",
    "   \n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    value = float(value) \n",
    "    \n",
    "    if date_key.endswith(\"_RH\"):  \n",
    "        date = date_key.split('_')[0]\n",
    "        \n",
    "        if date == current_date:\n",
    "            \n",
    "            min_relative_humidity = min(min_relative_humidity, value)\n",
    "        else:\n",
    "            \n",
    "            if current_date is not None:\n",
    "                print(f\"{current_date}\\t{min_relative_humidity}\")\n",
    "            \n",
    "            \n",
    "            current_date = date\n",
    "            min_relative_humidity = value\n",
    "\n",
    "\n",
    "if current_date:\n",
    "    print(f\"{current_date}\\t{min_relative_humidity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the mapper and reducer on the hadoop cluster\n",
    "\n",
    "[xxx@dsm1 weather]$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "    -files mapper.py,reducer_min.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer_min.py\" \\\n",
    "    -input /user/xxx/200707hourly.txt -output /user/xxx/200707_min_humidity\n",
    "\n",
    "\n",
    "# Copy the output from HDFS onto the headnode\n",
    "\n",
    "hadoop fs -copyToLocal /user/xxx/200707_min_humidity\n",
    "\n",
    "# Download the output from the headnode onto local machine\n",
    "\n",
    "scp -r xxx@dsm1.doc.gold.ac.uk:~/weather/200707_min_humidity /Users/jerid/Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9cc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1edb7699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t2.0\n",
      "20070702\t2.0\n",
      "20070703\t3.0\n",
      "20070704\t3.0\n",
      "20070705\t3.0\n",
      "20070706\t3.0\n",
      "20070707\t3.0\n",
      "20070708\t3.0\n",
      "20070709\t2.0\n",
      "20070710\t4.0\n",
      "20070711\t2.0\n",
      "20070712\t4.0\n",
      "20070713\t3.0\n",
      "20070714\t5.0\n",
      "20070715\t6.0\n",
      "20070716\t6.0\n",
      "20070717\t4.0\n",
      "20070718\t4.0\n",
      "20070719\t63.0\n"
     ]
    }
   ],
   "source": [
    "file = open('weather_final_results/200707_min_humidity/part-00000', 'r', encoding='UTF-8')\n",
    "\n",
    "for line in file.readlines():\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a388fb8",
   "metadata": {},
   "source": [
    "## 2.3 Calculating daily mean Dew Point Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae797d6",
   "metadata": {},
   "source": [
    "Testing the reducer on a smaller file in jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bb23e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t6.3478260869565215\n",
      "20070702\t10.8\n",
      "20070703\t8.88888888888889\n",
      "20070704\t21.375\n",
      "20070705\t36.0\n"
     ]
    }
   ],
   "source": [
    "file = open(\"mapper_output.txt\", \"r\")\n",
    "\n",
    "current_date = None\n",
    "total_dpt = 0\n",
    "count_dpt = 0\n",
    "\n",
    "for line in file.readlines():\n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    value = float(value)\n",
    "    \n",
    "    if date_key.endswith(\"_DPT\"):  # I will only process dew point temp data\n",
    "        date = date_key.split('_')[0]\n",
    "        \n",
    "        if date == current_date:\n",
    "            total_dpt += value\n",
    "            count_dpt += 1\n",
    "        else:\n",
    "            if current_date:\n",
    "                # I will now output the mean for the previous date\n",
    "                print(f\"{current_date}\\t{total_dpt / count_dpt}\")\n",
    "            \n",
    "            current_date = date\n",
    "            total_dpt = value\n",
    "            count_dpt = 1\n",
    "\n",
    "if current_date:\n",
    "    print(f\"{current_date}\\t{total_dpt / count_dpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cab86",
   "metadata": {},
   "source": [
    "I will now create the py file within the cluster and copy in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257dc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch reducer_mean.py\n",
    "nano reducer_mean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_date = None\n",
    "total_dpt = 0\n",
    "count_dpt = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    value = float(value)\n",
    "    \n",
    "    if date_key.endswith(\"_DPT\"):  \n",
    "        date = date_key.split('_')[0]\n",
    "        \n",
    "        if date == current_date:\n",
    "            total_dpt += value\n",
    "            count_dpt += 1\n",
    "        else:\n",
    "            if current_date:\n",
    "                \n",
    "                print(f\"{current_date}\\t{total_dpt / count_dpt}\")\n",
    "            \n",
    "            current_date = date\n",
    "            total_dpt = value\n",
    "            count_dpt = 1\n",
    "\n",
    "# Now I now forget to output the last date after finishing all the lines\n",
    "if current_date:\n",
    "    print(f\"{current_date}\\t{total_dpt / count_dpt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a447059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the mapper and reducer on the hadoop cluster\n",
    "\n",
    "[xxx@dsm1 weather]$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "    -files mapper.py,reducer_mean.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer_mean.py\" \\\n",
    "    -input /user/xxx/200707hourly.txt -output /user/xxx/200707_mean_dew_point\n",
    "\n",
    "\n",
    "# Copy the output from HDFS onto the headnode\n",
    "\n",
    "hadoop fs -copyToLocal /user/xxx/200707_mean_dew_point\n",
    "\n",
    "# Download the output from the headnode onto local machine\n",
    "\n",
    "scp -r xxx@dsm1.doc.gold.ac.uk:~/weather/200707_mean_dew_point /Users/jerid/Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39c723df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t54.41121880800165\n",
      "20070702\t54.871995298266235\n",
      "20070703\t57.25788551401869\n",
      "20070704\t59.58217147553494\n",
      "20070705\t60.22808453421414\n",
      "20070706\t59.42598134131315\n",
      "20070707\t59.81553625223012\n",
      "20070708\t61.24157568604308\n",
      "20070709\t61.533953159938804\n",
      "20070710\t61.20380650277558\n",
      "20070711\t59.09537080497536\n",
      "20070712\t56.90508613858481\n",
      "20070713\t56.98782019452639\n",
      "20070714\t58.23986664327075\n",
      "20070715\t59.077835910326485\n",
      "20070716\t59.63978101121293\n",
      "20070717\t61.56257216164208\n",
      "20070718\t63.56921766292773\n",
      "20070719\t74.78666666666666\n"
     ]
    }
   ],
   "source": [
    "file = open('weather_final_results/200707_mean_dew_point/part-00000', 'r', encoding='UTF-8')\n",
    "\n",
    "for line in file.readlines():\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455f2e7",
   "metadata": {},
   "source": [
    "## 2.4 Calculating daily variance of Dew Point Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd6cf1",
   "metadata": {},
   "source": [
    "Testing the reducer on a smaller file in jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d75a83e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t104.83553875236292\n",
      "20070702\t55.36\n",
      "20070703\t42.65432098765432\n",
      "20070704\t69.06770833333333\n",
      "20070705\t0.0\n"
     ]
    }
   ],
   "source": [
    "file = open(\"mapper_output.txt\", \"r\")\n",
    "\n",
    "# Creating the function to calculate the variance\n",
    "def calculate_variance(values):\n",
    "    if len(values) == 0:\n",
    "        return None\n",
    "    mean = sum(values) / len(values)\n",
    "    variance = sum((x - mean) ** 2 for x in values) / len(values)\n",
    "    return variance\n",
    "\n",
    "current_date = None\n",
    "temps = []\n",
    "\n",
    "for line in file.readlines():\n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    if \"_DPT\" in date_key:\n",
    "        date = date_key.split('_')[0]\n",
    "        value = float(value)\n",
    "        \n",
    "        if current_date == date:\n",
    "            temps.append(value)\n",
    "        else:\n",
    "            if current_date is not None:\n",
    "               \n",
    "            # Calculating and printing the variance for the previous date\n",
    "                variance = calculate_variance(temps)\n",
    "                print(f\"{current_date}\\t{variance}\")\n",
    "            current_date = date\n",
    "            temps = [value]\n",
    "\n",
    "# Now I will output the last date\n",
    "if current_date is not None:\n",
    "    variance = calculate_variance(temps)\n",
    "    print(f\"{current_date}\\t{variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35029ac",
   "metadata": {},
   "source": [
    "I will now create the py file within the cluster and copy in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch reducer_var.py\n",
    "nano reducer_var.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "def calculate_variance(values):\n",
    "    if len(values) == 0:\n",
    "        return None\n",
    "    mean = sum(values) / len(values)\n",
    "    variance = sum((x - mean) ** 2 for x in values) / len(values)\n",
    "    return variance\n",
    "\n",
    "current_date = None\n",
    "temps = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    date_key, value = line.strip().split('\\t')\n",
    "    if \"_DPT\" in date_key:\n",
    "        date = date_key.split('_')[0]\n",
    "        value = float(value)\n",
    "        \n",
    "        if current_date == date:\n",
    "            temps.append(value)\n",
    "        else:\n",
    "            if current_date is not None:\n",
    "                \n",
    "                variance = calculate_variance(temps)\n",
    "                print(f\"{current_date}\\t{variance}\")\n",
    "            current_date = date\n",
    "            temps = [value]\n",
    "\n",
    "\n",
    "if current_date is not None:\n",
    "    variance = calculate_variance(temps)\n",
    "    print(f\"{current_date}\\t{variance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the mapper and reducer on the hadoop cluster\n",
    "\n",
    "[xxx@dsm1 weather]$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "    -files mapper.py,reducer_var.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer_var.py\" \\\n",
    "    -input /user/xxx/200707hourly.txt -output /user/xxx/200707_var_dew_point\n",
    "\n",
    "\n",
    "# Copy the output from HDFS onto the headnode\n",
    "\n",
    "hadoop fs -copyToLocal /user/jdawi001/200707_var_dew_point\n",
    "\n",
    "# Download the output from the headnode onto local machine\n",
    "\n",
    "scp -r xxx@dsm1.doc.gold.ac.uk:~/weather/200707_var_dew_point /Users/jerid/Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c3f1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701\t179.3196301999967\n",
      "20070702\t172.6328654575169\n",
      "20070703\t157.3550487999709\n",
      "20070704\t123.4225017157641\n",
      "20070705\t108.4531073742198\n",
      "20070706\t105.31909731045707\n",
      "20070707\t116.19401559309561\n",
      "20070708\t121.7921870716496\n",
      "20070709\t132.6581681197474\n",
      "20070710\t137.68216837540814\n",
      "20070711\t125.50425221523547\n",
      "20070712\t104.96666124415624\n",
      "20070713\t102.27951984757348\n",
      "20070714\t92.49086429757877\n",
      "20070715\t98.7039933057828\n",
      "20070716\t103.3323730293108\n",
      "20070717\t93.91535990097952\n",
      "20070718\t87.63282782753336\n",
      "20070719\t2.0078222222222233\n"
     ]
    }
   ],
   "source": [
    "file = open('weather_final_results/200707_var_dew_point/part-00000', 'r', encoding='UTF-8')\n",
    "\n",
    "for line in file.readlines():\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e6165",
   "metadata": {},
   "source": [
    "## 3. Correlation matrix that describes the monthly correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00014d67",
   "metadata": {},
   "source": [
    "I will create the mapper to include the dry bulb temp on smaller file in python and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0044dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file for reading and the mapper_output file for writing\n",
    "with open(\"200707hourly_tiny.txt\", \"r\") as file, open(\"mapper_output_incl_dry_bulb.txt\", \"w\") as outfile:\n",
    "    for line in file:\n",
    "        # I will split the line into columns\n",
    "        columns = line.strip().split(',')\n",
    "        \n",
    "        try:\n",
    "            # I will now extract necessary fields\n",
    "            date = columns[1]\n",
    "            wind_speed = float(columns[12])  \n",
    "            relative_humidity = float(columns[11].strip('%'))  \n",
    "            dew_point_temp = float(columns[9])  \n",
    "            dry_bulb_temp = float(columns[8])\n",
    "            \n",
    "            # I will emit data for wind speed analysis, relative humidity, etc \n",
    "            outfile.write(f\"{date}_WS\\t{wind_speed}\\n\")\n",
    "            \n",
    "            \n",
    "            outfile.write(f\"{date}_RH\\t{relative_humidity}\\n\")\n",
    "            \n",
    "            \n",
    "            outfile.write(f\"{date}_DPT\\t{dew_point_temp}\\n\")\n",
    "            \n",
    "            \n",
    "            outfile.write(f\"{date}_DBT\\t{dry_bulb_temp}\\n\")\n",
    "            \n",
    "        except ValueError:\n",
    "           \n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c1b9051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070701_WS\t3.0\n",
      "20070701_RH\t22.0\n",
      "20070701_DPT\t18.0\n",
      "20070701_DBT\t57.0\n",
      "20070701_WS\t3.0\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "\n",
    "with open(\"mapper_output_incl_dry_bulb.txt\", \"r\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        print(line.strip())\n",
    "        if i >= 4:  \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885e1fb",
   "metadata": {},
   "source": [
    "Since the above works I will proceed to creating the mapper in cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the mapper file using touch command\n",
    "touch mapper2.py\n",
    "\n",
    "# Opening the mapper file with nano and I will paste the code below and save\n",
    "nano mapper2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb4f34",
   "metadata": {},
   "source": [
    "Within my nano mapper file created I will copy the code below as Hadoop, mappers and reducers communicate with stdin and stdout and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "        columns = line.strip().split(',')\n",
    "        \n",
    "        try:\n",
    "           \n",
    "            date = columns[1]\n",
    "            wind_speed = float(columns[12])  \n",
    "            relative_humidity = float(columns[11].strip('%'))  \n",
    "            dew_point_temp = float(columns[9])  \n",
    "            dry_bulb_temp = float(columns[8])\n",
    "            \n",
    "            \n",
    "            print(f\"{date}_WS\\t{wind_speed}\\n\")\n",
    "            \n",
    "            \n",
    "            print(f\"{date}_RH\\t{relative_humidity}\\n\")\n",
    "            \n",
    "            \n",
    "            print(f\"{date}_DPT\\t{dew_point_temp}\\n\")\n",
    "            \n",
    "            \n",
    "            print(f\"{date}_DBT\\t{dry_bulb_temp}\\n\")\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8890cfa",
   "metadata": {},
   "source": [
    "## Reducer for Correlation Matrix Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143deb4",
   "metadata": {},
   "source": [
    "testing reducer in jupyter notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88219d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS and RH Correlation: -0.09107301779923976\n",
      "WS and DBT Correlation: -0.07858355336150848\n",
      "RH and DBT Correlation: -0.6505568727257489\n"
     ]
    }
   ],
   "source": [
    "file = open(\"mapper_output_incl_dry_bulb.txt\", \"r\")\n",
    "\n",
    "def mean(data):\n",
    "    return sum(data) / len(data)\n",
    "\n",
    "def variance(data, mean):\n",
    "    return sum((x - mean) ** 2 for x in data) / len(data)\n",
    "\n",
    "def covariance(data_x, mean_x, data_y, mean_y):\n",
    "    return sum((x - mean_x) * (y - mean_y) for x, y in zip(data_x, data_y)) / len(data_x)\n",
    "\n",
    "data_ws = []\n",
    "data_rh = []\n",
    "data_dpt = []\n",
    "data_dbt = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    key, value = line.strip().split('\\t')\n",
    "    value = float(value)\n",
    "    \n",
    "    if key.endswith(\"_WS\"):\n",
    "        data_ws.append(value)\n",
    "    elif key.endswith(\"_RH\"):\n",
    "        data_rh.append(value)\n",
    "    elif key.endswith(\"_DPT\"):\n",
    "        data_dpt.append(value)\n",
    "    elif key.endswith(\"_DBT\"):\n",
    "        data_dbt.append(value)\n",
    "        \n",
    "        \n",
    "mean_ws = mean(data_ws)\n",
    "mean_rh = mean(data_rh)\n",
    "mean_dpt = mean(data_dpt)\n",
    "mean_dbt = mean(data_dbt)\n",
    "\n",
    "\n",
    "def manual_variance(data, mean):\n",
    "    return sum((x - mean) ** 2 for x in data) / len(data)\n",
    "\n",
    "variance_ws = manual_variance(data_ws, mean_ws)\n",
    "variance_rh = manual_variance(data_rh, mean_rh)\n",
    "variance_dpt = manual_variance(data_dpt, mean_dpt)\n",
    "variance_dbt = manual_variance(data_dbt, mean_dbt)\n",
    "\n",
    "\n",
    "def manual_covariance(data_x, mean_x, data_y, mean_y):\n",
    "    return sum((x - mean_x) * (y - mean_y) for x, y in zip(data_x, data_y)) / len(data_x)\n",
    "\n",
    "covariance_ws_rh = manual_covariance(data_ws, mean_ws, data_rh, mean_rh)\n",
    "covariance_ws_dbt = manual_covariance(data_ws, mean_ws, data_dbt, mean_dbt)\n",
    "covariance_rh_dbt = manual_covariance(data_rh, mean_rh, data_dbt, mean_dbt)\n",
    "\n",
    "\n",
    "correlation_ws_rh = covariance_ws_rh / (variance_ws ** 0.5 * variance_rh ** 0.5)\n",
    "correlation_ws_dbt = covariance_ws_dbt / (variance_ws ** 0.5 * variance_dbt ** 0.5)\n",
    "correlation_rh_dbt = covariance_rh_dbt / (variance_rh ** 0.5 * variance_dbt ** 0.5)\n",
    "\n",
    "\n",
    "print(f\"WS and RH Correlation: {correlation_ws_rh}\")\n",
    "print(f\"WS and DBT Correlation: {correlation_ws_dbt}\")\n",
    "print(f\"RH and DBT Correlation: {correlation_rh_dbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8f56b",
   "metadata": {},
   "source": [
    "I will now create the py file within the cluster and copy in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch reducer_corr.py\n",
    "nano reducer_corr.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "def mean(data):\n",
    "    return sum(data) / len(data)\n",
    "\n",
    "def variance(data, mean):\n",
    "    return sum((x - mean) ** 2 for x in data) / len(data)\n",
    "\n",
    "def covariance(data_x, mean_x, data_y, mean_y):\n",
    "    return sum((x - mean_x) * (y - mean_y) for x, y in zip(data_x, data_y)) / len(data_x)\n",
    "\n",
    "data_ws = []\n",
    "data_rh = []\n",
    "data_dpt = []\n",
    "data_dbt = []\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split('\\t')\n",
    "    value = float(value)\n",
    "    \n",
    "    if key.endswith(\"_WS\"):\n",
    "        data_ws.append(value)\n",
    "    elif key.endswith(\"_RH\"):\n",
    "        data_rh.append(value)\n",
    "    elif key.endswith(\"_DPT\"):\n",
    "        data_dpt.append(value)\n",
    "    elif key.endswith(\"_DBT\"):\n",
    "        data_dbt.append(value)\n",
    "        \n",
    "        \n",
    "mean_ws = mean(data_ws)\n",
    "mean_rh = mean(data_rh)\n",
    "mean_dpt = mean(data_dpt)\n",
    "mean_dbt = mean(data_dbt)\n",
    "\n",
    "\n",
    "def manual_variance(data, mean):\n",
    "    return sum((x - mean) ** 2 for x in data) / len(data)\n",
    "\n",
    "variance_ws = manual_variance(data_ws, mean_ws)\n",
    "variance_rh = manual_variance(data_rh, mean_rh)\n",
    "variance_dpt = manual_variance(data_dpt, mean_dpt)\n",
    "variance_dbt = manual_variance(data_dbt, mean_dbt)\n",
    "\n",
    "\n",
    "def manual_covariance(data_x, mean_x, data_y, mean_y):\n",
    "    return sum((x - mean_x) * (y - mean_y) for x, y in zip(data_x, data_y)) / len(data_x)\n",
    "\n",
    "covariance_ws_rh = manual_covariance(data_ws, mean_ws, data_rh, mean_rh)\n",
    "covariance_ws_dbt = manual_covariance(data_ws, mean_ws, data_dbt, mean_dbt)\n",
    "covariance_rh_dbt = manual_covariance(data_rh, mean_rh, data_dbt, mean_dbt)\n",
    "\n",
    "\n",
    "correlation_ws_rh = covariance_ws_rh / (variance_ws ** 0.5 * variance_rh ** 0.5)\n",
    "correlation_ws_dbt = covariance_ws_dbt / (variance_ws ** 0.5 * variance_dbt ** 0.5)\n",
    "correlation_rh_dbt = covariance_rh_dbt / (variance_rh ** 0.5 * variance_dbt ** 0.5)\n",
    "\n",
    "\n",
    "print(f\"WS and RH Correlation: {correlation_ws_rh}\")\n",
    "print(f\"WS and DBT Correlation: {correlation_ws_dbt}\")\n",
    "print(f\"RH and DBT Correlation: {correlation_rh_dbt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the mapper and reducer on the hadoop cluster\n",
    "\n",
    "[xxx@dsm1 weather]$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "    -files mapper2.py,reducer_corr.py \\\n",
    "    -mapper \"python3 mapper2.py\" \\\n",
    "    -reducer \"python3 reducer_corr.py\" \\\n",
    "    -input /user/jdawi001/200707hourly.txt -output /user/xxx/200707_corr_matrix\n",
    "\n",
    "\n",
    "# Copy the output from HDFS onto the headnode\n",
    "\n",
    "hadoop fs -copyToLocal /user/xxx/200707_corr_matrix\n",
    "\n",
    "# Download the output from the headnode onto local machine\n",
    "\n",
    "scp -r xxx@dsm1.doc.gold.ac.uk:~/weather/200707_corr_matrix /Users/jerid/Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3b284cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS and RH Correlation: -0.3441714843390527\n",
      "WS and DBT Correlation: 0.5652201413259447\n",
      "RH and DBT Correlation: -0.09552092151475639\n"
     ]
    }
   ],
   "source": [
    "file = open('weather_final_results/200707_corr_matrix/part-00000', 'r', encoding='UTF-8')\n",
    "\n",
    "for line in file.readlines():\n",
    "    print(line.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
